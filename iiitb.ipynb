{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9836550,"sourceType":"datasetVersion","datasetId":6033840}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"4a8cf6aa-1e3d-4e89-9235-3d86fc0e01b2","cell_type":"code","source":"import os\n\nimport numpy as np\n\nimport torch\n\nfrom torchvision import datasets\n\nfrom torchvision.transforms import ToTensor\n\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader\n\nimport torch.nn as nn\n\nfrom torch import optim\n\nfrom torch.autograd import Variable\n\nfrom torch.utils.data.sampler import Sampler\n\nfrom sklearn.metrics import normalized_mutual_info_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"954934d1-cd81-406f-a592-c132025d4279","cell_type":"code","source":"!pip install faiss-gpu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"73c9a71d-ce4e-4fe2-bbed-17eb750b5a54","cell_type":"code","source":"class UnifLabelSampler(Sampler):\n    \"\"\"Samples elements uniformely accross pseudolabels.\n        Args:\n            N (int): size of returned iterator.\n            images_lists: dict of key (target), value (list of data with this target)\n    \"\"\"\n\n    def __init__(self, N, images_lists):\n        self.N = N\n        self.images_lists = images_lists\n        self.indexes = self.generate_indexes_epoch()\n\n    def generate_indexes_epoch(self):\n        nmb_non_empty_clusters = 0\n        for i in range(len(self.images_lists)):\n            if len(self.images_lists[i]) != 0:\n                nmb_non_empty_clusters += 1\n\n        size_per_pseudolabel = int(self.N / nmb_non_empty_clusters) + 1\n        res = np.array([])\n\n        for i in range(len(self.images_lists)):\n            # skip empty clusters\n            if len(self.images_lists[i]) == 0:\n                continue\n            indexes = np.random.choice(\n                self.images_lists[i],\n                size_per_pseudolabel,\n                replace=(len(self.images_lists[i]) <= size_per_pseudolabel)\n            )\n            res = np.concatenate((res, indexes))\n\n        np.random.shuffle(res)\n        res = list(res.astype('int'))\n        if len(res) >= self.N:\n            return res[:self.N]\n        res += res[: (self.N - len(res))]\n        return res\n\n    def __iter__(self):\n        return iter(self.indexes)\n\n    def __len__(self):\n        return len(self.indexes)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2e3f7734-e154-47c7-b9db-40082b48f62d","cell_type":"code","source":"import sys\n\n# Add the directory containing `clus.py` to the Python path\nsys.path.append('/kaggle/input/iiitbdata')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"588b97c3-ca2b-4e85-a02b-519e32eaeb3f","cell_type":"code","source":"import clus\n\ndeepcluster = clus.__dict__[\"Kmeans\"](10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f39dc56d-9fc1-49fc-b652-0a6f3f8585d7","cell_type":"code","source":"# Device configuration\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndevice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e64c73c0-989c-49d2-bf6e-4ffc0d110f22","cell_type":"code","source":"train_data = datasets.MNIST(\n\n    root = 'data',\n\n    train = True,                         \n\n    transform = ToTensor(), \n\n    download = True,            \n\n)\n\ntest_data = datasets.MNIST(\n\n    root = 'data', \n\n    train = False, \n\n    transform = ToTensor()\n\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1b6672d5-4f0a-4270-ad03-216a8fc13b7d","cell_type":"code","source":"print(train_data)\n\nprint(train_data.data.size())\n\nprint(train_data.targets.size())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2194a773-58c1-425f-be80-b82e63567af1","cell_type":"code","source":"# figure = plt.figure(figsize=(10, 8))\n\n# cols, rows = 5, 5\n\n# for i in range(1, cols * rows + 1):\n\n#     sample_idx = torch.randint(len(train_data), size=(1,)).item()\n\n#     img, label = train_data[sample_idx]\n\n#     figure.add_subplot(rows, cols, i)\n\n#     plt.title(label)\n\n#     plt.axis(\"off\")\n\n#     plt.imshow(img.squeeze(), cmap=\"gray\")\n\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a2c935b4-8613-41dc-8a6c-e95e18f78dc7","cell_type":"code","source":"#data loader\n\nbatch_size = 100\n\ntrain_loader = torch.utils.data.DataLoader(train_data, \n\n                                          batch_size=batch_size, \n\n                                          shuffle=True, \n\n                                          num_workers=1)\n\n    \n\ntest_loader = torch.utils.data.DataLoader(test_data, \n\n                                          batch_size=batch_size, \n\n                                          shuffle=True, \n\n                                          num_workers=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f9ff42d3-b4f6-4606-bc5d-713c79bce3fb","cell_type":"code","source":"true_labels_list = train_data.targets.numpy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"62e9a5fe-9bcb-4062-a04d-662fd657bf2a","cell_type":"code","source":"# Define the Convolutional Neural Network model\n\nclass CNN(nn.Module):\n\n    def __init__(self):\n\n        super(CNN, self).__init__()\n\n        self.cnt = 1\n\n        self.conv1 = nn.Sequential(         \n\n            nn.Conv2d(\n\n                in_channels=1,              \n\n                out_channels=16,            \n\n                kernel_size=5,              \n\n                stride=1,                   \n\n                padding=2,                  \n\n            ),                              \n\n            nn.ReLU(),                      \n\n            nn.MaxPool2d(kernel_size=2),    \n\n        )\n\n        self.conv2 = nn.Sequential(         \n\n            nn.Conv2d(16, 32, 5, 1, 2),     \n\n            nn.ReLU(),                      \n\n            nn.MaxPool2d(2),                \n\n        )\n\n        self.fc1 = nn.Sequential(nn.Dropout(0.5),\n\n                            nn.Linear(32 * 7 * 7, 256),\n\n                            nn.ReLU(inplace=True),\n\n                            nn.Dropout(0.5),\n\n                            nn.Linear(256, 256),\n\n                            nn.ReLU(inplace=True))\n\n        \n\n        # fully connected layer, output 10 classes\n\n        self.out = nn.Linear(256, 10)\n\n    def forward(self, x):\n\n        if self.cnt:\n\n            print(f'input={x.shape}')\n\n        x = self.conv1(x)\n\n        if self.cnt:\n\n            print(f'first CNN output={x.shape}')\n\n        x = self.conv2(x)\n\n        if self.cnt:\n\n            print(f'second CNN output={x.shape}')\n\n        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n\n        x = x.view(x.size(0), -1)    \n\n        if self.cnt:\n\n            print(f'flatten output={x.shape}')\n\n        x = self.fc1(x)  \n\n        if self.cnt:\n\n            print(f'fc1={x.shape}')\n\n        if self.out:\n\n            x = self.out(x)\n\n        \n\n        if self.cnt:\n\n            print(f'output layer output={x.shape}')\n\n            self.cnt = 0\n\n        return  x    # return x for visualization","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f26c58f3-01a0-4dd2-a073-041052489716","cell_type":"code","source":"cnn = CNN()\n\ncnn.out=None\n\nprint(cnn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b5b3708c-a9c9-4108-a38d-90165090aa10","cell_type":"code","source":"#loss function\n\nloss_func = nn.CrossEntropyLoss()   \n\nloss_func","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b8a95e1d-976d-4639-a2e1-e52f54d925f4","cell_type":"code","source":"# optimizer\n\noptimizer = optim.Adam(cnn.parameters(), lr = 0.001)   \n\noptimizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f3e55b78-1f27-4be6-8076-e5cbea48b382","cell_type":"code","source":"def compute_features(dataloader, N):\n\n    cnn.eval()\n\n    for i, (images, labels) in enumerate(dataloader):\n\n        b_x = Variable(images)   # batch x\n\n        with torch.no_grad():\n\n            aux = cnn(b_x)\n\n        aux = aux.cpu().numpy()\n\n  \n\n        if i == 0:\n\n            features = np.zeros((N, aux.shape[1]), dtype='float32')\n\n\n\n        aux = aux.astype('float32')\n\n        if i < len(dataloader) - 1:\n\n            features[i * batch_size: (i + 1) * batch_size] = aux\n\n        else:\n\n            # special treatment for final batch\n\n            features[i * batch_size:] = aux\n\n    return features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"33c0715f-25a2-4e24-85e7-98470e72aef3","cell_type":"code","source":"class ReassignedDataset():\n\n    \"\"\"A dataset where the new images labels are given in argument.\n\n    Args:\n\n        image_indexes (list): list of data indexes\n\n        pseudolabels (list): list of labels for each data\n\n        dataset (list): list of tuples with paths to images\n\n        transform (callable, optional): a function/transform that takes in\n\n                                        an PIL image and returns a\n\n                                        transformed version\n\n    \"\"\"\n\n\n\n    def __init__(self, image_indexes, pseudolabels, dataset, transform=None):\n\n        self.imgs = self.make_dataset(image_indexes, pseudolabels, dataset)\n\n        self.transform = transform\n\n\n\n    def make_dataset(self,image_indexes, pseudolabels, dataset):\n\n        label_to_idx = {label: idx for idx, label in enumerate(set(pseudolabels))}\n\n        images = []\n\n        for j, idx in enumerate(image_indexes):\n\n            img, _ = dataset[idx]  # Retrieve the image from the dataset\n\n            pseudolabel = label_to_idx[pseudolabels[j]]\n\n            images.append((img, pseudolabel))\n\n        return images\n\n\n\n    def __getitem__(self, index):\n\n        \"\"\"\n\n        Args:\n\n            index (int): index of data\n\n        Returns:\n\n            tuple: (image, pseudolabel) where pseudolabel is the cluster of the index datapoint\n\n        \"\"\"\n\n        img, pseudolabel = self.imgs[index]\n\n        # img is already a tensor, so no need to load it with pil_loader\n\n        if self.transform is not None:\n\n            img = self.transform(img)\n\n        return img, pseudolabel\n\n\n\n\n\n    def __len__(self):\n\n        return len(self.imgs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c2d0072b-0a08-445a-8815-0a0b0193cf41","cell_type":"code","source":"def cluster_assign(images_lists, dataset):\n\n    \"\"\"Creates a dataset from clustering, with clusters as labels.\n\n    Args:\n\n        images_lists (list of list): for each cluster, the list of image indexes\n\n                                    belonging to this cluster\n\n        dataset (list): initial dataset\n\n    Returns:\n\n        ReassignedDataset(torch.utils.data.Dataset): a dataset with clusters as\n\n                                                     labels\n\n    \"\"\"\n\n    assert images_lists is not None\n\n    pseudolabels = []\n\n    image_indexes = []\n\n    for cluster, images in enumerate(images_lists):\n\n        image_indexes.extend(images)\n\n        pseudolabels.extend([cluster] * len(images))\n\n    return ReassignedDataset(image_indexes, pseudolabels, dataset, None)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"361e0be7-18f5-46d4-b429-52a696839aed","cell_type":"code","source":"def per_epoch(epochs, num_epochs, cnn, loader, train):\n\n\n\n    if train:\n\n        cnn.train()\n\n        optimizer_tl = torch.optim.SGD(cnn.out.parameters(),lr=0.01, weight_decay=10**-5)\n\n    else:\n\n        cnn.eval()\n\n        \n\n    # Train the model\n\n    total_step = len(loader)\n\n    #print(total_step)\n\n    total_loss = 0.0\n\n    total_acc = 0.0\n\n        \n\n    for i, (images, labels) in enumerate(loader):\n\n        # gives batch data, normalize x when iterate train_loader\n\n        b_x = Variable(images)   # batch x\n\n        b_y = Variable(labels)   # batch y\n\n        output = cnn(b_x) \n\n        print(output.shape)\n\n        loss = loss_func(output, b_y)\n\n        total_loss+= loss.item()\n\n        pred_y = torch.max(output, 1)[1].data.squeeze()\n\n        accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n\n        total_acc += accuracy\n\n        if train:\n\n            # clear gradients for this training step   \n\n            optimizer.zero_grad()\n\n            optimizer_tl.zero_grad()\n\n        \n\n            # backpropagation, compute gradients \n\n            loss.backward()\n\n            #for name, param in cnn.named_parameters():\n\n                #if param.grad is not None:\n\n                    #print(f'Gradient of {name}: {param.grad.abs().mean().item()}')\n\n            # apply gradients             \n\n            optimizer.step() \n\n            optimizer_tl.step()\n\n        \n\n        if (i+1) % 100 == 0:\n\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f} Acc: {:.4f} ' \n\n                   .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(), accuracy))\n\n    return (total_loss/total_step), (total_acc/total_step)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5172b8d4-c4d6-479e-9a6a-3536c86c3dfd","cell_type":"code","source":"tr_loss_epoch,  val_loss_epoch,  tr_acc_epoch,  val_acc_epoch = [], [], [], []\n\nnmi_list1=[]\n\nnmi_list=[]\n\nprev_assignments = None\n\nnum_epochs = 100\n\nleast_loss = 9999\n\nlogdir='mnist_output'\n\nprint(logdir)\n\n# Create the output folder\n\ntry:\n\n    os.stat(logdir)\n\nexcept:\n\n    os.makedirs(logdir)\n\n\n\nfor epoch in range(0, num_epochs):\n\n    cnn.out=None\n\n    train_features = compute_features(train_loader, len(train_data))\n\n    print(train_features.shape)\n\n    test_features=compute_features(test_loader, len(test_data))\n\n    # cluster the feature\n\n    clustering_loss = deepcluster.cluster(train_features,test_features, verbose=False)\n\n    \n\n    # assign pseudo-labels\n\n    train_dataset = cluster_assign(deepcluster.images_lists, train_data)\n\n   \n\n    # uniformly sample per target\n\n    sampler = UnifLabelSampler(int(1 * len(train_dataset)), deepcluster.images_lists)\n\n\n\n    train_dataloader = torch.utils.data.DataLoader(\n\n        train_dataset,\n\n        batch_size=batch_size,\n\n        num_workers=1,\n\n        sampler=sampler,\n\n        pin_memory=True,\n\n    )\n\n   \n\n    # assign pseudo-labels\n\n    test_dataset = cluster_assign(deepcluster.images_lists1, test_data)\n\n\n\n    sampler = UnifLabelSampler(int(1 * len(test_dataset)), deepcluster.images_lists1)\n\n\n\n    test_dataloader = torch.utils.data.DataLoader(\n\n        test_dataset,\n\n        batch_size=batch_size,\n\n        num_workers=1,\n\n        sampler=sampler,\n\n        pin_memory=True,\n\n    )\n\n    cnn.out= nn.Linear(256, 10)\n\n    \n\n    tr_loss, tr_acc = per_epoch(epoch, num_epochs, cnn, train_dataloader, train=True)\n\n    tr_loss_epoch.append(tr_loss), tr_acc_epoch.append(tr_acc)\n\n    val_loss, val_acc = per_epoch(epoch, num_epochs, cnn, test_dataloader, train=False)\n\n    val_loss_epoch.append(val_loss), val_acc_epoch.append(val_acc)\n\n    print ('Epoch {}, Train Loss: {:.4f} train Acc: {:.4f} Val Loss: {:.4f} Val Acc: {:.4f} ' \n\n                   .format(epoch, tr_loss, tr_acc, val_loss, val_acc))\n\n\n\n    train_file = open(logdir + '/train_log.txt',\"a\")\n\n    train_file.write(\"Epoch {}: train: [loss-{:.6f} error-{:.6f} ], val: [loss-{:.6f} error-{:.6f}]\\n\".                 \n\n              format(epoch, tr_loss, tr_acc, val_loss, val_acc))\n\n    train_file.close()\n\n\n\n    if val_loss < least_loss:\n\n        min_loss_epoch = epoch\n\n        torch.save(cnn.state_dict(), logdir+'/best_loss_model.pth')\n\n        least_loss = val_loss \n\n\n\n    current_assignments = np.zeros(len(train_data))\n\n    for cluster_id, indices in enumerate(deepcluster.images_lists):\n\n        for index in indices:\n\n            current_assignments[index] = cluster_id\n\n\n\n    # Calculate NMI between true labels and current assignments\n\n    nmi1 = normalized_mutual_info_score(true_labels_list, current_assignments)\n\n    nmi_list1.append(nmi1)\n\n    print(f'Epoch {epoch}, NMI 2 {nmi1}')\n\n\n\n    # Calculate NMI if there is a previous assignment\n\n    if prev_assignments is not None:\n\n        nmi = normalized_mutual_info_score(prev_assignments, current_assignments)\n\n        nmi_list.append(nmi)\n\n        print(f'Epoch {epoch}, NMI {nmi}')\n\n\n\n    train_file = open(logdir + '/train_log.txt',\"a\")\n\n    train_file.write(\"NMI between true labels and current assignments {}\\n\".format(nmi1))\n\n    if prev_assignments is not None:\n\n        train_file.write(\"NMI between previous and current assignments {}\\n\".format(nmi))\n\n    train_file.close()\n\n    \n\n    # Update previous assignments\n\n    prev_assignments = current_assignments","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"96bbb663-27d8-4735-9129-c7f4a679ffcd","cell_type":"code","source":"\n\n            print(\"=> loading checkpoint \")\n\n            checkpoint = torch.load('mnist_output/best_loss_model.pth')\n\n\n\n            # remove top_layer parameters from checkpoint\n\n            for keys in checkpoint:\n\n                print(keys)\n\n           \n\n            print(checkpoint['fc1.1.weight'] )\n\n           \n\n            print(\"=> loaded checkpoint  (epoch {})\"\n\n                  .format( checkpoint['epoch']))\n\n        \n\n          ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"fb0c4fd3-74f5-4e70-9213-9dc0c32fa487","cell_type":"code","source":"# tr_loss_epoch,  val_loss_epoch,  tr_acc_epoch,  val_acc_epoch = [], [], [], []\n\n# num_epochs = 10\n\n# for epoch in range(num_epochs):\n\n#     tr_loss, tr_acc = per_epoch(epoch, num_epochs, cnn, train_loader, train=True)\n\n#     tr_loss_epoch.append(tr_loss), tr_acc_epoch.append(tr_acc)\n\n#     val_loss, val_acc = per_epoch(epoch, num_epochs, cnn, test_loader, train=False)\n\n#     val_loss_epoch.append(val_loss), val_acc_epoch.append(val_acc)\n\n#     print ('Epoch {}, Train Loss: {:.4f} train Acc: {:.4f} Val Loss: {:.4f} Val Acc: {:.4f} ' \n\n#                    .format(epoch, tr_loss, tr_acc, val_loss, val_acc))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"dd14fd36-df87-4b5c-95ac-329fe5c10482","cell_type":"code","source":"plt.figure()\n\nplt.plot(tr_loss_epoch, label='training')\n\nplt.plot(val_loss_epoch, label='validation')\n\nplt.xlabel('Epochs')\n\nplt.ylabel('Loss')\n\nplt.legend()\n\nplt.savefig(logdir + \"/loss.png\")\n\n\n\nplt.figure()\n\nplt.plot(tr_acc_epoch, label='training')\n\nplt.plot(val_acc_epoch, label='validation')\n\nplt.xlabel('Epochs')\n\nplt.ylabel('Accuracy')\n\nplt.legend()\n\nplt.savefig(logdir + \"/acc.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"11d3eaf4-d5ad-4756-ba9a-7116d7d9c944","cell_type":"code","source":"# Plot NMI over epochs\n\nplt.figure()\n\nplt.plot(nmi_list, label='NMI over epochs')\n\nplt.xlabel('Epoch')\n\nplt.ylabel('NMI')\n\nplt.title('NMI Stability Over Time')\n\nplt.legend()\n\nplt.show()\n\n\n\n# Plot NMI over epochs\n\nplt.figure()\n\nplt.plot(nmi_list1, label='NMI over epochs')\n\nplt.xlabel('Epoch')\n\nplt.ylabel('NMI')\n\nplt.title('NMI between true labels and cluster assignments over epochs')\n\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9814cc6d-7a2a-4d31-84ff-0857c21250b1","cell_type":"code","source":"def test():\n\n    # Test the model\n\n    cnn.eval()\n\n    with torch.no_grad():\n\n        correct = 0\n\n        total = 0\n\n        for images, labels in test_loader:\n\n            test_output, last_layer = cnn(images)\n\n            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n\n            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n\n    return accuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"21ae9875-c956-480a-b1c9-df043e043fdd","cell_type":"code","source":"accuracy = test()\n\nprint('Test Accuracy of the model on the 10000 test images: %.2f' % accuracy)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4997ddcc-dc45-49c7-a742-6e2c33492fc6","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2a15d447-87d5-48cb-bfd5-a3197f46ef45","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}